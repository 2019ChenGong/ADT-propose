
@article{mnih_human-level_2015,
	title = {Human-level control through deep reinforcement learning},
	volume = {518},
	issn = {0028-0836, 1476-4687},
	doi = {10.1038/nature14236},
	language = {en},
	number = {7540},
	urldate = {2020-02-27},
	journal = {Nature},
	author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and others},
	year = {2015},
	pages = {529--533},
	file = {Mnih 等。 - 2015 - Human-level control through deep reinforcement lea.pdf:D\:\\zotero\\storage\\W6JJC8ZP\\Mnih 等。 - 2015 - Human-level control through deep reinforcement lea.pdf:application/pdf}
}

@inproceedings{bellemare_distributional_2017,
	address = {Sydney, NSW, Australia},
	series = {{ICML}'17},
	title = {A {Distributional} {Perspective} on {Reinforcement} {Learning}},
	abstract = {In this paper we argue for the fundamental importance of the value distribution: the distribution of the random return received by a reinforcement learning agent. This is in contrast to the common approach to reinforcement learning which models the expectation of this return, or value. Although there is an established body of literature studying the value distribution, thus far it has always been used for a specific purpose such as implementing risk-aware behaviour. We begin with theoretical results in both the policy evaluation and control settings, exposing a significant distributional instability in the latter. We then use the distributional perspective to design a new algorithm which applies Bellman's equation to the learning of approximate value distributions. We evaluate our algorithm using the suite of games from the Arcade Learning Environment. We obtain both state-of-the-art results and anecdotal evidence demonstrating the importance of the value distribution in approximate reinforcement learning. Finally, we combine theoretical and empirical evidence to highlight the ways in which the value distribution impacts learning in the approximate setting.},
	urldate = {2020-02-27},
	booktitle = {Proceedings of the 34th {International} {Conference} on {Machine} {Learning} - {Volume} 70},
	publisher = {JMLR.org},
	author = {Bellemare, Marc G. and Dabney, Will and Munos, Rémi},
	year = {2017},
	keywords = {DRL, ICML},
	pages = {449--458},
	file = {Bellemare et al_2017_A Distributional Perspective on Reinforcement Learning.pdf:D\:\\zotero\\storage\\RKMQZ7UE\\Bellemare et al_2017_A Distributional Perspective on Reinforcement Learning.pdf:application/pdf}
}

@inproceedings{liu_stein_2016,
	title = {Stein {Variational} {Gradient} {Descent}: {A} {General} {Purpose} {Bayesian} {Inference} {Algorithm}},
	shorttitle = {Stein {Variational} {Gradient} {Descent}},
	urldate = {2020-02-27},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 29},
	publisher = {Curran Associates, Inc.},
	author = {Liu, Qiang and Wang, Dilin},
	editor = {Lee, D. D. and Sugiyama, M. and Luxburg, U. V. and Guyon, I. and Garnett, R.},
	year = {2016},
	keywords = {Stein},
	pages = {2378--2386},
	file = {Liu_Wang_2016_Stein Variational Gradient Descent.pdf:D\:\\zotero\\storage\\EHKQXK7N\\Liu_Wang_2016_Stein Variational Gradient Descent.pdf:application/pdf}
}

@inproceedings{tessler_stabilizing_2020,
	title = {Stabilizing {Deep} {Reinforcement} {Learning} with {Conservative} {Updates}},
	url = {http://arxiv.org/abs/1910.01062},
	abstract = {In recent years, advances in deep learning have enabled the application of reinforcement learning algorithms in complex domains. However, they lack the theoretical guarantees which are present in the tabular setting and suffer from many stability and reproducibility problems {\textbackslash}citep\{henderson2018deep\}. In this work, we suggest a simple approach for improving stability and providing probabilistic performance improvement in off-policy actor-critic deep reinforcement learning regimes. Experiments on continuous action spaces, in the MuJoCo control suite, show that our proposed method reduces the variance of the process and improves the overall performance.},
	urldate = {2020-02-28},
	booktitle = {{arXiv}:1910.01062 [cs, stat]},
	author = {Tessler, Chen and Merlis, Nadav and Mannor, Shie},
	year = {2020},
	note = {arXiv: 1910.01062},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Tessler et al_2020_Stabilizing Deep Reinforcement Learning with Conservative Updates.pdf:D\:\\zotero\\storage\\LCX4SNKL\\Tessler et al_2020_Stabilizing Deep Reinforcement Learning with Conservative Updates.pdf:application/pdf}
}

@inproceedings{dabney_distributional_2018,
	title = {Distributional reinforcement learning with quantile regression},
	booktitle = {Thirty-{Second} {AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Dabney, Will and Rowland, Mark and Bellemare, Marc G. and others},
	year = {2018},
	keywords = {QRDQN},
	file = {Dabney et al_2018_Distributional reinforcement learning with quantile regression.pdf:D\:\\zotero\\storage\\F8BI9DCY\\Dabney 等。 - 2018 - Distributional reinforcement learning with quantil.pdf:application/pdf}
}

@inproceedings{yang_fully_2019,
	title = {Fully {Parameterized} {Quantile} {Function} for {Distributional} {Reinforcement} {Learning}},
	url = {http://papers.nips.cc/paper/8850-fully-parameterized-quantile-function-for-distributional-reinforcement-learning.pdf},
	urldate = {2020-03-02},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 32},
	publisher = {Curran Associates, Inc.},
	author = {Yang, Derek and Zhao, Li and Lin, Zichuan and Qin, Tao and Bian, Jiang and Liu, Tie-Yan},
	editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and Alché-Buc, F. d{\textbackslash}textquotesingle and Fox, E. and Garnett, R.},
	year = {2019},
	keywords = {FQF},
	pages = {6193--6202},
	file = {Yang et al_2019_Fully Parameterized Quantile Function for Distributional Reinforcement Learning.pdf:D\:\\zotero\\storage\\C6UEAYMN\\Yang et al_2019_Fully Parameterized Quantile Function for Distributional Reinforcement Learning.pdf:application/pdf}
}

@inproceedings{dabney_implicit_2018,
	title = {Implicit {Quantile} {Networks} for {Distributional} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1806.06923},
	abstract = {In this work, we build on recent advances in distributional reinforcement learning to give a generally applicable, flexible, and state-of-the-art distributional variant of DQN. We achieve this by using quantile regression to approximate the full quantile function for the state-action return distribution. By reparameterizing a distribution over the sample space, this yields an implicitly defined return distribution and gives rise to a large class of risk-sensitive policies. We demonstrate improved performance on the 57 Atari 2600 games in the ALE, and use our algorithm's implicitly defined distributions to study the effects of risk-sensitive policies in Atari games.},
	urldate = {2020-03-02},
	booktitle = {{arXiv}:1806.06923 [cs, stat]},
	author = {Dabney, Will and Ostrovski, Georg and Silver, David and Munos, Rémi},
	year = {2018},
	note = {arXiv: 1806.06923},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Dabney et al_2018_Implicit Quantile Networks for Distributional Reinforcement Learning.pdf:D\:\\zotero\\storage\\CFPU4RWH\\Dabney et al_2018_Implicit Quantile Networks for Distributional Reinforcement Learning.pdf:application/pdf;arXiv Fulltext PDF:D\:\\zotero\\storage\\DGCQPXLP\\Dabney 等。 - 2018 - Implicit Quantile Networks for Distributional Rein.pdf:application/pdf}
}

@book{bass_real_2013,
	address = {Lieu de publication inconnu},
	title = {Real analysis for graduate students},
	isbn = {978-1-4818-6914-0},
	language = {en},
	publisher = {R.F. Bass},
	author = {Bass, Richard F},
	year = {2013},
	note = {OCLC: 864441260},
	file = {Bass - 2013 - Real analysis for graduate students.pdf:D\:\\zotero\\storage\\EL34HYHQ\\Bass - 2013 - Real analysis for graduate students.pdf:application/pdf}
}

@article{rowland_analysis_2018,
	title = {An {Analysis} of {Categorical} {Distributional} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1802.08163},
	abstract = {Distributional approaches to value-based reinforcement learning model the entire distribution of returns, rather than just their expected values, and have recently been shown to yield state-of-the-art empirical performance. This was demonstrated by the recently proposed C51 algorithm, based on categorical distributional reinforcement learning (CDRL) [Bellemare et al., 2017]. However, the theoretical properties of CDRL algorithms are not yet well understood. In this paper, we introduce a framework to analyse CDRL algorithms, establish the importance of the projected distributional Bellman operator in distributional RL, draw fundamental connections between CDRL and the Cram{\textbackslash}'er distance, and give a proof of convergence for sample-based categorical distributional reinforcement learning algorithms.},
	urldate = {2020-03-15},
	journal = {arXiv:1802.08163 [stat]},
	author = {Rowland, Mark and Bellemare, Marc G. and Dabney, Will and Munos, Rémi and Teh, Yee Whye},
	month = feb,
	year = {2018},
	note = {arXiv: 1802.08163},
	keywords = {Statistics - Machine Learning},
	file = {Rowland et al_2018_An Analysis of Categorical Distributional Reinforcement Learning.pdf:D\:\\zotero\\storage\\U496EXF7\\Rowland et al_2018_An Analysis of Categorical Distributional Reinforcement Learning.pdf:application/pdf}
}

@article{silver_mastering_2016,
	title = {Mastering the game of {Go} with deep neural networks and tree search},
	volume = {529},
	number = {7587},
	journal = {nature},
	author = {Silver, David and Huang, Aja and Maddison, Chris J. and others},
	year = {2016},
	note = {Publisher: Nature Publishing Group},
	pages = {484},
	file = {Silver et al_2016_Mastering the game of Go with deep neural networks and tree search.pdf:D\:\\zotero\\storage\\AYLVQD7C\\Silver et al_2016_Mastering the game of Go with deep neural networks and tree search.pdf:application/pdf}
}

@article{silver_mastering_2017,
	title = {Mastering the game of go without human knowledge},
	volume = {550},
	number = {7676},
	journal = {Nature},
	author = {Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian},
	year = {2017},
	note = {Publisher: Nature Publishing Group},
	pages = {354--359}
}

@article{levine_end--end_2016,
	title = {End-to-end training of deep visuomotor policies},
	volume = {17},
	number = {1},
	journal = {The Journal of Machine Learning Research},
	author = {Levine, Sergey and Finn, Chelsea and Darrell, Trevor and Abbeel, Pieter},
	year = {2016},
	note = {Publisher: JMLR. org},
	pages = {1334--1373},
	file = {Levine et al_2016_End-to-end training of deep visuomotor policies.pdf:D\:\\zotero\\storage\\2BNAYVZI\\Levine et al_2016_End-to-end training of deep visuomotor policies.pdf:application/pdf}
}

@inproceedings{mao_resource_2016,
	title = {Resource management with deep reinforcement learning},
	booktitle = {Proceedings of the 15th {ACM} {Workshop} on {Hot} {Topics} in {Networks}},
	author = {Mao, Hongzi and Alizadeh, Mohammad and Menache, Ishai and Kandula, Srikanth},
	year = {2016},
	pages = {50--56}
}

@article{jaques_tuning_2017,
	title = {Tuning recurrent neural networks with reinforcement learning},
	author = {Jaques, Natasha and Gu, Shixiang and Turner, Richard E. and Eck, Douglas},
	year = {2017},
	file = {Jaques et al_2017_Tuning recurrent neural networks with reinforcement learning.pdf:D\:\\zotero\\storage\\IK2C2VHU\\Jaques et al_2017_Tuning recurrent neural networks with reinforcement learning.pdf:application/pdf}
}

@inproceedings{mnih_asynchronous_2016,
	title = {Asynchronous methods for deep reinforcement learning},
	booktitle = {International conference on machine learning},
	author = {Mnih, Volodymyr and Badia, Adria Puigdomenech and others},
	year = {2016},
	keywords = {A2c},
	pages = {1928--1937},
	file = {Mnih et al_2016_Asynchronous methods for deep reinforcement learning.pdf:D\:\\zotero\\storage\\9WIKAHQB\\Mnih et al_2016_Asynchronous methods for deep reinforcement learning.pdf:application/pdf}
}

@article{chen_generative_2018,
	title = {Generative adversarial user model for reinforcement learning based recommendation system},
	journal = {arXiv preprint arXiv:1812.10613},
	author = {Chen, Xinshi and Li, Shuang and Li, Hui and Jiang, Shaohua and Qi, Yuan and Song, Le},
	year = {2018},
	file = {Chen et al_2018_Generative adversarial user model for reinforcement learning based.pdf:D\:\\zotero\\storage\\AMLCLKJN\\Chen et al_2018_Generative adversarial user model for reinforcement learning based.pdf:application/pdf}
}

@article{chen_generative_2018-1,
	title = {Generative adversarial user model for reinforcement learning based recommendation system},
	journal = {arXiv preprint arXiv:1812.10613},
	author = {Chen, Xinshi and Li, Shuang and Li, Hui and Jiang, Shaohua and Qi, Yuan and Song, Le},
	year = {2018},
	file = {Chen et al_2018_Generative adversarial user model for reinforcement learning based.pdf:D\:\\zotero\\storage\\5QRAQ38Y\\Chen et al_2018_Generative adversarial user model for reinforcement learning based.pdf:application/pdf}
}

@inproceedings{shi_virtual-taobao_2019,
	title = {Virtual-taobao: {Virtualizing} real-world online retail environment for reinforcement learning},
	volume = {33},
	shorttitle = {Virtual-taobao},
	booktitle = {Proceedings of the {AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Shi, Jing-Cheng and Yu, Yang and Da, Qing and Chen, Shi-Yong and Zeng, An-Xiang},
	year = {2019},
	pages = {4902--4909},
	file = {Shi et al_2019_Virtual-taobao.pdf:D\:\\zotero\\storage\\PKFB7DQK\\Shi et al_2019_Virtual-taobao.pdf:application/pdf}
}

@article{zhang_deep_2019,
	title = {Deep learning based recommender system: {A} survey and new perspectives},
	volume = {52},
	shorttitle = {Deep learning based recommender system},
	number = {1},
	journal = {ACM Computing Surveys (CSUR)},
	author = {Zhang, Shuai and Yao, Lina and Sun, Aixin and Tay, Yi},
	year = {2019},
	note = {Publisher: ACM New York, NY, USA},
	pages = {1--38},
	file = {Zhang et al_2019_Deep learning based recommender system.pdf:D\:\\zotero\\storage\\4ZKLCB4E\\Zhang et al_2019_Deep learning based recommender system.pdf:application/pdf}
}

@article{bengio_machine_2018,
	title = {Machine {Learning} for {Combinatorial} {Optimization}: a {Methodological} {Tour} d'{Horizon}},
	shorttitle = {Machine {Learning} for {Combinatorial} {Optimization}},
	journal = {arXiv preprint arXiv:1811.06128},
	author = {Bengio, Yoshua and Lodi, Andrea and Prouvost, Antoine},
	year = {2018},
	file = {Bengio et al_2018_Machine Learning for Combinatorial Optimization.pdf:D\:\\zotero\\storage\\I5SMU56Y\\Bengio et al_2018_Machine Learning for Combinatorial Optimization.pdf:application/pdf}
}

@article{segler_planning_2018,
	title = {Planning chemical syntheses with deep neural networks and symbolic {AI}},
	volume = {555},
	number = {7698},
	journal = {Nature},
	author = {Segler, Marwin HS and Preuss, Mike and Waller, Mark P.},
	year = {2018},
	note = {Publisher: Nature Publishing Group},
	pages = {604--610},
	file = {Segler et al_2018_Planning chemical syntheses with deep neural networks and symbolic AI.pdf:D\:\\zotero\\storage\\FFL98U2B\\Segler et al_2018_Planning chemical syntheses with deep neural networks and symbolic AI.pdf:application/pdf}
}

@inproceedings{tang_deep_2019,
	title = {A deep value-network based approach for multi-driver order dispatching},
	booktitle = {Proceedings of the 25th {ACM} {SIGKDD} international conference on knowledge discovery \& data mining},
	author = {Tang, Xiaocheng and Qin, Zhiwei and Zhang, Fan and Wang, Zhaodong and Xu, Zhe and Ma, Yintai and Zhu, Hongtu and Ye, Jieping},
	year = {2019},
	pages = {1780--1790}
}

@inproceedings{henderson_deep_2018,
	title = {Deep reinforcement learning that matters},
	booktitle = {Thirty-{Second} {AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Henderson, Peter and Islam, Riashat and Bachman, Philip and Pineau, Joelle and Precup, Doina and Meger, David},
	year = {2018},
	file = {Henderson et al_2018_Deep reinforcement learning that matters.pdf:D\:\\zotero\\storage\\Y3HZ2DZ9\\Henderson et al_2018_Deep reinforcement learning that matters.pdf:application/pdf}
}

@article{islam_reproducibility_2017,
	title = {Reproducibility of benchmarked deep reinforcement learning tasks for continuous control},
	journal = {arXiv preprint arXiv:1708.04133},
	author = {Islam, Riashat and Henderson, Peter and Gomrokchi, Maziar and Precup, Doina},
	year = {2017},
	file = {Islam et al_2017_Reproducibility of benchmarked deep reinforcement learning tasks for continuous.pdf:D\:\\zotero\\storage\\995BF9CH\\Islam et al_2017_Reproducibility of benchmarked deep reinforcement learning tasks for continuous.pdf:application/pdf}
}

@book{sutton_reinforcement_2018,
	title = {Reinforcement learning: {An} introduction},
	shorttitle = {Reinforcement learning},
	publisher = {MIT press},
	author = {Sutton, Richard S. and Barto, Andrew G.},
	year = {2018},
	file = {Sutton_Barto_2018_Reinforcement learning.pdf:D\:\\zotero\\storage\\H3YSRU5Q\\Sutton_Barto_2018_Reinforcement learning.pdf:application/pdf}
}

@article{ivanov_modern_2019,
	title = {Modern {Deep} {Reinforcement} {Learning} {Algorithms}},
	journal = {arXiv preprint arXiv:1906.10025},
	author = {Ivanov, Sergey and D'yakonov, Alexander},
	year = {2019},
	keywords = {阅读},
	file = {Ivanov_D'yakonov_2019_Modern Deep Reinforcement Learning Algorithms.pdf:D\:\\zotero\\storage\\QU9ZNHVM\\Ivanov_D'yakonov_2019_Modern Deep Reinforcement Learning Algorithms.pdf:application/pdf}
}

@inproceedings{sutton_policy_2000,
	title = {Policy gradient methods for reinforcement learning with function approximation},
	booktitle = {Advances in neural information processing systems},
	author = {Sutton, Richard S. and McAllester, David A. and Singh, Satinder P. and Mansour, Yishay},
	year = {2000},
	pages = {1057--1063},
	file = {Sutton et al_2000_Policy gradient methods for reinforcement learning with function approximation.pdf:D\:\\zotero\\storage\\PKBKRTHD\\Sutton et al_2000_Policy gradient methods for reinforcement learning with function approximation.pdf:application/pdf}
}

@inproceedings{van_hasselt_deep_2016,
	title = {Deep reinforcement learning with double q-learning},
	booktitle = {Thirtieth {AAAI} conference on artificial intelligence},
	author = {Van Hasselt, Hado and Guez, Arthur and Silver, David},
	year = {2016},
	file = {Van Hasselt et al_2016_Deep reinforcement learning with double q-learning.pdf:D\:\\zotero\\storage\\ZJ8NNDLE\\Van Hasselt et al_2016_Deep reinforcement learning with double q-learning.pdf:application/pdf}
}

@inproceedings{hessel_rainbow_2018,
	title = {Rainbow: {Combining} improvements in deep reinforcement learning},
	shorttitle = {Rainbow},
	booktitle = {Thirty-{Second} {AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Hessel, Matteo and Modayil, Joseph and others},
	year = {2018}
}

@article{schulman_high-dimensional_2015,
	title = {High-dimensional continuous control using generalized advantage estimation},
	journal = {arXiv preprint arXiv:1506.02438},
	author = {Schulman, John and Moritz, Philipp and Levine, Sergey and Jordan, Michael and Abbeel, Pieter},
	year = {2015},
	file = {Schulman et al_2015_High-dimensional continuous control using generalized advantage estimation.pdf:D\:\\zotero\\storage\\RUDK3MIH\\Schulman et al_2015_High-dimensional continuous control using generalized advantage estimation.pdf:application/pdf}
}

@article{watkins_q-learning_1992,
	title = {Q-learning},
	volume = {8},
	number = {3-4},
	journal = {Machine learning},
	author = {Watkins, Christopher JCH and Dayan, Peter},
	year = {1992},
	note = {Publisher: Springer},
	pages = {279--292},
	file = {Watkins_Dayan_1992_Q-learning.pdf:D\:\\zotero\\storage\\KIE8DTFW\\Watkins_Dayan_1992_Q-learning.pdf:application/pdf}
}

@article{mnih_playing_2013,
	title = {Playing atari with deep reinforcement learning},
	journal = {arXiv preprint arXiv:1312.5602},
	author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and others},
	year = {2013},
	file = {Mnih et al_2013_Playing atari with deep reinforcement learning.pdf:D\:\\zotero\\storage\\X8YAPRES\\Mnih et al_2013_Playing atari with deep reinforcement learning.pdf:application/pdf}
}

@article{wang_dueling_2015,
	title = {Dueling network architectures for deep reinforcement learning},
	journal = {arXiv preprint arXiv:1511.06581},
	author = {Wang, Ziyu and Schaul, Tom and Hessel, Matteo and others},
	year = {2015},
	file = {Wang et al_2015_Dueling network architectures for deep reinforcement learning.pdf:D\:\\zotero\\storage\\I48QFSFV\\Wang et al_2015_Dueling network architectures for deep reinforcement learning.pdf:application/pdf}
}

@article{schaul_prioritized_2015,
	title = {Prioritized experience replay},
	journal = {arXiv preprint arXiv:1511.05952},
	author = {Schaul, Tom and Quan, John and Antonoglou, Ioannis and Silver, David},
	year = {2015},
	file = {Schaul et al_2015_Prioritized experience replay.pdf:D\:\\zotero\\storage\\RNDZH523\\Schaul et al_2015_Prioritized experience replay.pdf:application/pdf}
}

@article{horgan_distributed_2018,
	title = {Distributed prioritized experience replay},
	journal = {arXiv preprint arXiv:1803.00933},
	author = {Horgan, Dan and Quan, John and Budden, David and Barth-Maron, Gabriel and Hessel, Matteo and Van Hasselt, Hado and Silver, David},
	year = {2018},
	file = {Horgan et al_2018_Distributed prioritized experience replay.pdf:D\:\\zotero\\storage\\V2PAT9CN\\Horgan et al_2018_Distributed prioritized experience replay.pdf:application/pdf}
}

@inproceedings{schulman_trust_2015,
	title = {Trust region policy optimization},
	booktitle = {International conference on machine learning},
	author = {Schulman, John and Levine, Sergey and Abbeel, Pieter and others},
	year = {2015},
	pages = {1889--1897},
	file = {Schulman et al_2015_Trust region policy optimization.pdf:D\:\\zotero\\storage\\R6H4LS3T\\Schulman et al_2015_Trust region policy optimization.pdf:application/pdf}
}

@article{schulman_proximal_2017,
	title = {Proximal policy optimization algorithms},
	journal = {arXiv preprint arXiv:1707.06347},
	author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and others},
	year = {2017},
	keywords = {PPO},
	file = {Schulman et al_2017_Proximal policy optimization algorithms.pdf:D\:\\zotero\\storage\\ANVM79B3\\Schulman et al_2017_Proximal policy optimization algorithms.pdf:application/pdf}
}

@inproceedings{wu_scalable_2017,
	title = {Scalable trust-region method for deep reinforcement learning using kronecker-factored approximation},
	booktitle = {Advances in neural information processing systems},
	author = {Wu, Yuhuai and Mansimov, Elman and Grosse, Roger B. and Liao, Shun and Ba, Jimmy},
	year = {2017},
	pages = {5279--5288},
	file = {Wu et al_2017_Scalable trust-region method for deep reinforcement learning using.pdf:D\:\\zotero\\storage\\M3DLDQ8B\\Wu et al_2017_Scalable trust-region method for deep reinforcement learning using.pdf:application/pdf}
}

@inproceedings{konda_actor-critic_2000,
	title = {Actor-critic algorithms},
	booktitle = {Advances in neural information processing systems},
	author = {Konda, Vijay R. and Tsitsiklis, John N.},
	year = {2000},
	pages = {1008--1014},
	file = {Konda_Tsitsiklis_2000_Actor-critic algorithms.pdf:D\:\\zotero\\storage\\WCB7BE74\\Konda_Tsitsiklis_2000_Actor-critic algorithms.pdf:application/pdf}
}

@inproceedings{silver_deterministic_2014,
	title = {Deterministic policy gradient algorithms},
	author = {Silver, David and Lever, Guy and Heess, Nicolas and Degris, Thomas and Wierstra, Daan and Riedmiller, Martin},
	year = {2014}
}

@article{lillicrap_continuous_2015,
	title = {Continuous control with deep reinforcement learning},
	journal = {arXiv preprint arXiv:1509.02971},
	author = {Lillicrap, Timothy P. and Hunt, Jonathan J. and Pritzel, Alexander and others},
	year = {2015},
	keywords = {DDPG},
	file = {Lillicrap et al_2015_Continuous control with deep reinforcement learning.pdf:D\:\\zotero\\storage\\8KM6HEDG\\Lillicrap et al_2015_Continuous control with deep reinforcement learning.pdf:application/pdf}
}

@article{fujimoto_addressing_2018,
	title = {Addressing function approximation error in actor-critic methods},
	journal = {arXiv preprint arXiv:1802.09477},
	author = {Fujimoto, Scott and Van Hoof, Herke and Meger, David},
	year = {2018},
	keywords = {TD3},
	file = {Fujimoto et al_2018_Addressing function approximation error in actor-critic methods.pdf:D\:\\zotero\\storage\\GE6HAJ7L\\Fujimoto et al_2018_Addressing function approximation error in actor-critic methods.pdf:application/pdf}
}

@misc{noauthor_continuous_nodate,
	title = {Continuous {Control} {With} {Deep} {Reinforcement} {Learning}, - {Google} 学术搜索},
	url = {https://scholar.google.com.hk/scholar?hl=zh-CN&as_sdt=0%2C5&q=Continuous+Control+With+Deep+Reinforcement+Learning%2C&btnG=},
	urldate = {2020-04-05}
}

@inproceedings{silver_deterministic_2014-1,
	title = {Deterministic policy gradient algorithms},
	author = {Silver, David and Lever, Guy and Heess, Nicolas and Degris, Thomas and Wierstra, Daan and Riedmiller, Martin},
	year = {2014},
	keywords = {DPG},
	file = {Silver et al_2014_Deterministic policy gradient algorithms.pdf:D\:\\zotero\\storage\\XYVJK2UK\\Silver et al_2014_Deterministic policy gradient algorithms.pdf:application/pdf}
}

@article{haarnoja_soft_2018,
	title = {Soft actor-critic: {Off}-policy maximum entropy deep reinforcement learning with a stochastic actor},
	shorttitle = {Soft actor-critic},
	journal = {arXiv preprint arXiv:1801.01290},
	author = {Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
	year = {2018},
	file = {Haarnoja et al_2018_Soft actor-critic.pdf:D\:\\zotero\\storage\\BC6S5DUC\\Haarnoja et al_2018_Soft actor-critic.pdf:application/pdf}
}

@misc{noauthor_white_2020_04_08_boltzmann_machine_nodate,
	title = {White\_2020\_04\_08\_Boltzmann\_Machine},
	url = {https://www.overleaf.com/project/5e8d7890702985000121cf58},
	abstract = {An online LaTeX editor that's easy to use. No installation, real-time collaboration, version control, hundreds of LaTeX templates, and more.},
	language = {en},
	urldate = {2020-04-15},
	note = {Library Catalog: www.overleaf.com}
}

@article{murphy_combining_2000,
	title = {Combining belief functions when evidence conflicts},
	volume = {29},
	number = {1},
	journal = {Decision support systems},
	author = {Murphy, Catherine K.},
	year = {2000},
	note = {Publisher: Elsevier},
	pages = {1--9}
}

@article{liu_stein_2017,
	title = {Stein {Variational} {Policy} {Gradient}},
	abstract = {Policy gradient methods have been successfully applied to many complex reinforcement learning problems. However, policy gradient methods suffer from high variance, slow convergence, and inefficient exploration. In this work, we introduce a maximum entropy policy optimization framework which explicitly encourages parameter exploration, and show that this framework can be reduced to a Bayesian inference problem. We then propose a novel Stein variational policy gradient method (SVPG) which combines existing policy gradient methods and a repulsive functional to generate a set of diverse but well-behaved policies. SVPG is robust to initialization and can easily be implemented in a parallel manner. On continuous control problems, we find that implementing SVPG on top of REINFORCE and advantage actor-critic algorithms improves both average return and data efficiency.},
	urldate = {2020-04-18},
	journal = {arXiv:1704.02399},
	author = {Liu, Yang and Ramachandran, Prajit and Liu, Qiang and Peng, Jian},
	year = {2017},
	keywords = {Computer Science - Machine Learning},
	}
}

@misc{noauthor_stein_nodate,
	title = {Stein variational gradient descent with matrix-valued kernels - {Google} 搜索},
	url = {https://www.google.com/search?rlz=1C1CHWL_zh-CNUS880US880&sxsrf=ALeKk00exVDLoZZSTPUk65rIKqH2GB6S0w%3A1587214219787&ei=i_eaXuvNL7ex0PEPqfuiyA4&q=Stein+variational+gradient+descent+with+matrix-valued+kernels&oq=Stein+variational+gradient+descent+with+matrix-valued+kernels&gs_lcp=CgZwc3ktYWIQAzIECCMQJzIECAAQHjoCCABQnpGFAViekYUBYP2ShQFoAHAAeACAAZkEiAH9B5IBBTQtMS4xmAEAoAECoAEBqgEHZ3dzLXdpeg&sclient=psy-ab&ved=0ahUKEwirv4LegfLoAhW3GDQIHam9COkQ4dUDCAw&uact=5},
	urldate = {2020-04-18}
}

@inproceedings{wang_stein_2019,
	title = {Stein {Variational} {Gradient} {Descent} {With} {Matrix}-{Valued} {Kernels}},
	url = {http://papers.nips.cc/paper/8998-stein-variational-gradient-descent-with-matrix-valued-kernels.pdf},
	urldate = {2020-04-18},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 32},
	publisher = {Curran Associates, Inc.},
	author = {Wang, Dilin and Tang, Ziyang and Bajaj, Chandrajit and Liu, Qiang},
	editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and Alché-Buc, F. d{\textbackslash}textquotesingle and Fox, E. and Garnett, R.},
	year = {2019},
	pages = {7836--7846},
	file = {Wang et al_2019_Stein Variational Gradient Descent With Matrix-Valued Kernels.pdf:D\:\\zotero\\storage\\QDUD9AWQ\\Wang et al_2019_Stein Variational Gradient Descent With Matrix-Valued Kernels.pdf:application/pdf}
}

@inproceedings{liu_stein_2018,
	title = {Stein {Variational} {Gradient} {Descent} as {Moment} {Matching}},
	urldate = {2020-04-18},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 31},
	publisher = {Curran Associates, Inc.},
	author = {Liu, Qiang and Wang, Dilin},
	editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and Cesa-Bianchi, N. and Garnett, R.},
	year = {2018},
	pages = {8854--8863},
	file = {Liu_Wang_2018_Stein Variational Gradient Descent as Moment Matching.pdf:D\:\\zotero\\storage\\S6N2UC33\\Liu_Wang_2018_Stein Variational Gradient Descent as Moment Matching.pdf:application/pdf}
}

@inproceedings{liu_stein_2017-1,
	title = {Stein {Variational} {Gradient} {Descent} as {Gradient} {Flow}},
	url = {http://papers.nips.cc/paper/6904-stein-variational-gradient-descent-as-gradient-flow.pdf},
	urldate = {2020-04-18},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 30},
	publisher = {Curran Associates, Inc.},
	author = {Liu, Qiang},
	editor = {Guyon, I. and Luxburg, U. V. and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
	year = {2017},
	pages = {3115--3123},
	file = {Liu_2017_Stein Variational Gradient Descent as Gradient Flow.pdf:D\:\\zotero\\storage\\VHVNIESI\\Liu_2017_Stein Variational Gradient Descent as Gradient Flow.pdf:application/pdf}
}

@article{pearce_bayesian_2018,
	title = {Bayesian inference with anchored ensembles of neural networks, and application to reinforcement learning},
	journal = {arXiv preprint arXiv:1805.11324},
	author = {Pearce, Tim and Anastassacos, Nicolas and Zaki, Mohamed and Neely, Andy},
	year = {2018},
	file = {Pearce et al_2018_Bayesian inference with anchored ensembles of neural networks, and application.pdf:D\:\\zotero\\storage\\HIICAXNK\\Pearce et al_2018_Bayesian inference with anchored ensembles of neural networks, and application.pdf:application/pdf}
}

@article{pearce_uncertainty_2018,
	title = {Uncertainty in neural networks: {Bayesian} ensembling},
	shorttitle = {Uncertainty in neural networks},
	journal = {arXiv preprint arXiv:1810.05546},
	author = {Pearce, Tim and Zaki, Mohamed and Brintrup, Alexandra and Anastassacos, Nicolas and Neely, Andy},
	year = {2018},
	file = {Pearce et al_2018_Uncertainty in neural networks.pdf:D\:\\zotero\\storage\\898AXFFG\\Pearce et al_2018_Uncertainty in neural networks.pdf:application/pdf}
}

@article{bellemare2013arcade,
  title={The arcade learning environment: An evaluation platform for general agents},
  author={Bellemare, Marc G and Naddaf, Yavar and Veness, Joel and Bowling, Michael},
  journal={Journal of Artificial Intelligence Research},
  volume={47},
  pages={253--279},
  year={2013}
}

@inproceedings{tang2015line,
  title={Line: Large-scale information network embedding},
  author={Tang, Jian and Qu, Meng and Wang, Mingzhe and Zhang, Ming and Yan, Jun and Mei, Qiaozhu},
  booktitle={Proceedings of the 24th international conference on world wide web},
  pages={1067--1077},
  year={2015}
}

@inproceedings{nachum2018data,
  title={Data-efficient hierarchical reinforcement learning},
  author={Nachum, Ofir and Gu, Shixiang Shane and others},
  booktitle={Advances in Neural Information Processing Systems},
  pages={3303--3313},
  year={2018}
}

@article{espeholt2018impala,
  title={Impala: Scalable distributed deep-rl with importance weighted actor-learner architectures},
  author={Espeholt, Lasse and Soyer, Hubert and Munos, Remi and others},
  journal={arXiv preprint arXiv:1802.01561},
  year={2018}
}

@article{vinyals2019grandmaster,
title = {Grandmaster level in StarCraft II using multi-agent reinforcement learning},
author = {Vinyals, Oriol and Babuschkin, Igor and others},
year = {2019},
month = {11},
pages = {350-354},
volume = {575},
journal = {Nature},
}

@article{silver2016mastering,
author = {Silver, David and Huang, Aja and Maddison, Christopher and Guez, Arthur and Sifre, Laurent and Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
year = {2016},
month = {01},
pages = {484-489},
title = {Mastering the game of Go with deep neural networks and tree search},
volume = {529},
journal = {Nature},
doi = {10.1038/nature16961}
}
@inproceedings{zhao2020state,
author = {Zhao, Jian and Zhou, Wengang and Zhao, Tianyu and Zhou, Yun and others},
year = {2020},
month = {07},
pages = {1-6},
title = {State Representation Learning For Effective Deep Reinforcement Learning},
doi = {10.1109/ICME46284.2020.9102924}
}

@InProceedings{pmlr-v70-arjovsky17a,
title={{W}asserstein Generative Adversarial Networks},
author={Martin, Arjovsky and Soumith, Chintala and L{\'e}on, Bottou},
title = {Proceedings of the 34th International Conference on Machine Learning},
pages = {214--223},
year = {2017},
volume = {70},
}

@article{brockman2016openai,
  title={Openai gym},
  author={Brockman, Greg and Cheung, Vicki and Pettersson, Ludwig and others},
  journal={arXiv preprint arXiv:1606.01540},
  year={2016}
}

@inproceedings{todorov2012mujoco,
  title={Mujoco: A physics engine for model-based control},
  author={Todorov, Emanuel and Erez, Tom and Tassa, Yuval},
  booktitle={2012 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  pages={5026--5033},
  year={2012},
  organization={IEEE}
}

@inproceedings{gong2020stable,
  title={Stable Training of Bellman Error in Reinforcement Learning},
  author={Gong, Chen and Bai, Yunpeng and Hou, Xinwen and Ji, Xiaohui},
  booktitle={International Conference on Neural Information Processing},
  pages={439--448},
  year={2020},
  organization={Springer}
}

@article{survey,
	title={Offline reinforcement learning: Tutorial, review, and perspectives on open problems},
	author={Levine, Sergey and Kumar, Aviral and Tucker, George and Fu, Justin},
	journal={arXiv preprint arXiv:2005.01643},
	year={2020}
}


@book{rl,
	title={Reinforcement learning: An introduction},
	author={Sutton, Richard S and Barto, Andrew G},
	year={2018},
	publisher={MIT press}
}

@inproceedings{ddqn,
	title={Deep reinforcement learning with double q-learning},
	author={Van Hasselt, Hado and Guez, Arthur and Silver, David},
	booktitle={Thirtieth AAAI conference on artificial intelligence},
	year={2016}
}

@inproceedings{bcq,
	title={Off-policy deep reinforcement learning without exploration},
	author={Fujimoto, Scott and Meger, David and Precup, Doina},
	booktitle={International Conference on Machine Learning},
	pages={2052--2062},
	year={2019}
}

@inproceedings{bear,
	title={Stabilizing off-policy q-learning via bootstrapping error reduction},
	author={Kumar, Aviral and Fu, Justin and Soh, Matthew and Tucker, George and Levine, Sergey},
	booktitle={Advances in Neural Information Processing Systems},
	pages={11784--11794},
	year={2019}
}

@article{morel,
	title={MOReL: Model-Based Offline Reinforcement Learning},
	author={Kidambi, Rahul and Rajeswaran, Aravind and Netrapalli, Praneeth and Joachims, Thorsten},
	journal={arXiv preprint arXiv:2005.05951},
	year={2020}
}

@article{mopo,
	title={MOPO: Model-based Offline Policy Optimization},
	author={Yu, Tianhe and Thomas, Garrett and Yu, Lantao and Ermon, Stefano and Zou, James and Levine, Sergey and Finn, Chelsea and Ma, Tengyu},
	journal={arXiv preprint arXiv:2005.13239},
	year={2020}
}

@article{brac,
	title={Behavior regularized offline reinforcement learning},
	author={Wu, Yifan and Tucker, George and Nachum, Ofir},
	journal={arXiv preprint arXiv:1911.11361},
	year={2019}
}

@article{crr,
	title={Critic Regularized Regression},
	author={Wang, Ziyu and Novikov, Alexander and {\.Z}o{\l}na, Konrad and Springenberg, Jost Tobias and Reed, Scott and Shahriari, Bobak and Siegel, Noah and Merel, Josh and Gulcehre, Caglar and Heess, Nicolas and others},
	journal={arXiv preprint arXiv:2006.15134},
	year={2020}
}

@article{minatar,
	title={Minatar: An atari-inspired testbed for thorough and reproducible reinforcement learning experiments},
	author={Young, Kenny and Tian, Tian},
	journal={arXiv preprint arXiv:1903.03176},
	year={2019}
}

@inproceedings{givan00,
	title={Bounded parameter Markov decision processes},
	author={Givan, Robert and Leach, Sonia and Dean, Thomas},
	booktitle={European Conference on Planning},
	pages={234--246},
	year={1997},
	organization={Springer}
}

@article{taleghan,
	title={PAC optimal MDP planning with application to invasive species management},
	author={Taleghan, Majid Alkaee and Dietterich, Thomas G and Crowley, Mark and Hall, Kim and Albers, H Jo},
	journal={The Journal of Machine Learning Research},
	volume={16},
	number={1},
	pages={3877--3903},
	year={2015},
	publisher={JMLR. org}
}

@article{weissman03,
	title={Inequalities for the L1 deviation of the empirical distribution},
	author={Weissman, Tsachy and Ordentlich, Erik and Seroussi, Gadiel and Verdu, Sergio and Weinberger, Marcelo J},
	journal={Hewlett-Packard Labs, Tech. Rep},
	year={2003}
}

@article{ernst05treebased,
	author = {Damien Ernst and Pierre Geurts and Louis Wehenkel},
	title = {Tree-based batch mode reinforcement learning},
	journal = {Journal of Machine Learning Research},
	year = {2005},
	volume = {6},
	pages = {503--556},
}

@article{lange12batch,
	title={Batch reinforcement learning},
	author={Lange, Sascha and Gabel, Thomas and Riedmiller, Martin},
	journal={Reinforcement learning},
	pages={45--73},
	year={2012},
}

@inproceedings{vanhasselt10double,
	title={Double {Q}-Learning},
	author={Hado van Hasselt},
	booktitle={Advances in Neural Information Processing Systems 23},
	year={2010}
}

@inproceedings{azar11speedy,
	title={Speedy {Q}-Learning},
	author={Mohammad G. Azar and R{\'e}mi Munos and Mohammad Gavamzadeh and Hilbert J. Kappen},
	booktitle={Advances in Neural Information Processing Systems},
	year={2011}
}

@article{azar12dynamic,
	title={Dynamic Policy Programming},
	author={Mohammad Gheshlaghi Azar and Vicenc G{\'o}mez and Hilbert J. Kappen},
	journal={Journal of Machine Learning Research},
	year={2012}
}

@inproceedings{lee13biascorrected,
	title={Bias-corrected Q-learning to control max-operator bias in Q-learning},
	author={Lee, Donghun and Defourny, Boris and Powell, Warren B},
	booktitle={2013 IEEE Symposium on Adaptive Dynamic Programming and Reinforcement Learning (ADPRL)},
	pages={93--99},
	year={2013},
	organization={IEEE}
}

@inproceedings{bellemare16increasing,
	title={Increasing the action gap: New operators for reinforcement learning},
	author={Bellemare, Marc G and Ostrovski, Georg and Guez, Arthur and Thomas, Philip S and Munos, R{\'e}mi},
	booktitle={Thirtieth AAAI Conference on Artificial Intelligence},
	year={2016}
}

@book{bandit,
	title={Bandit algorithms},
	author={Lattimore, Tor and Szepesv{\'a}ri, Csaba},
	year={2020},
	publisher={Cambridge University Press}
}


@article{ppo,
	title={Proximal policy optimization algorithms},
	author={Schulman, John and Wolski, Filip and Dhariwal, Prafulla and others},
	journal={arXiv preprint arXiv:1707.06347},
	year={2017}
}

@article{cql,
	title={Conservative Q-Learning for Offline Reinforcement Learning},
	author={Kumar, Aviral and Zhou, Aurick and Tucker, George and Levine, Sergey},
	journal={arXiv preprint arXiv:2006.04779},
	year={2020}
}

@inproceedings{riedmiller2005neural,
	title={Neural fitted Q iteration--first experiences with a data efficient neural reinforcement learning method},
	author={Riedmiller, Martin},
	booktitle={European Conference on Machine Learning},
	pages={317--328},
	year={2005},
	organization={Springer}
}

@inproceedings{antos2007value,
	title={Value-iteration based fitted policy iteration: learning with a single trajectory},
	author={Antos, Andr{\'a}s and Szepesv{\'a}ri, Csaba and Munos, R{\'e}mi},
	booktitle={2007 IEEE international symposium on approximate dynamic programming and reinforcement learning},
	pages={330--337},
	year={2007},
	organization={IEEE}
}

@inproceedings{fox16taming,
	title={Taming the Noise in Reinforcement Learning via Soft Updates},
	author={Roy Fox and Ari Pakman and Naftali Tishby},
	booktitle={Conference on Uncertainty in Artificial Intelligence},
	year={2016}
}

@inproceedings{nachum17bridging,
	title={Bridging the gap between value and policy based reinforcement learning},
	author={Ofir Nachum and Mohammad Norouzi and Kelvin Xu and Dale Schuurmans},
	booktitle={Advances in Neural Information Processing Systems},
	year={2017}
}

@inproceedings{brown19extrapolating,
	title={Extrapolating Beyond Suboptimal Demonstrations via Inverse Reinforcement Learning from Observations},
	author={Daniel S. Brown and Wonjoon Goo and Prabhat Nagarajan and Scott Niekum},
	booktitle={Proceedings of the International Conference on Machine Learning},
	year={2019}
}

@article{dadashi19value,
	title={The Value Function Polytope in Reinforcement Learning},
	author={Robert Dadashi and Adrien Ali Ta{\"i}ga and Nicolas Le Roux and Dale Schuurmans and Marc G. Bellemare},
	journal={Proceedings of the International Conference on Machine Learning},
	year={2019}
}

@article{dqn,
	title={Human-level control through deep reinforcement learning},
	author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and others},
	journal={Nature},
	volume={518},
	number={7540},
	pages={529--533},
	numpages={5},
	year={2015},
	publisher={Nature Publishing Group}
}

@article{fedus2020revisiting,
	title={Revisiting Fundamentals of Experience Replay},
	author={Fedus, William and Ramachandran, Prajit and Agarwal, Rishabh and Bengio, Yoshua and Larochelle, Hugo and Rowland, Mark and Dabney, Will},
	journal={arXiv preprint arXiv:2007.06700},
	year={2020}
}

@article{swaminathan2015batch,
	title={Batch learning from logged bandit feedback through counterfactual risk minimization},
	author={Swaminathan, Adith and Joachims, Thorsten},
	journal={The Journal of Machine Learning Research},
	volume={16},
	number={1},
	pages={1731--1755},
	year={2015},
	publisher={JMLR. org}
}

@article{maurer2009empirical,
	title={Empirical Bernstein bounds and sample variance penalization},
	author={Maurer, Andreas and Pontil, Massimiliano},
	journal={arXiv preprint arXiv:0907.3740},
	year={2009}
}

@article{nilim2005robust,
	title={Robust control of Markov decision processes with uncertain transition matrices},
	author={Nilim, Arnab and El Ghaoui, Laurent},
	journal={Operations Research},
	volume={53},
	number={5},
	pages={780--798},
	year={2005},
	publisher={INFORMS}
}

@article{wiesemann2013robust,
	title={Robust Markov decision processes},
	author={Wiesemann, Wolfram and Kuhn, Daniel and Rustem, Ber{\c{c}}},
	journal={Mathematics of Operations Research},
	volume={38},
	number={1},
	pages={153--183},
	year={2013},
	publisher={INFORMS}
}

@inproceedings{trpo,
	title={Trust region policy optimization},
	author={Schulman, John and Levine, Sergey and  others},
	booktitle={International conference on machine learning},
	pages={1889--1897},
	year={2015}
}

@article{klc,
	title={Way off-policy batch deep reinforcement learning of implicit human preferences in dialog},
	author={Jaques, Natasha and Ghandeharioun, Asma and Shen, Judy Hanwen and Ferguson, Craig and Lapedriza, Agata and Jones, Noah and Gu, Shixiang and Picard, Rosalind},
	journal={arXiv preprint arXiv:1907.00456},
	year={2019}
}

@inproceedings{petrik16,
	title={Safe policy improvement by minimizing robust baseline regret},
	author={Ghavamzadeh, Mohammad and Petrik, Marek and Chow, Yinlam},
	booktitle={Advances in Neural Information Processing Systems},
	pages={2298--2306},
	year={2016}
}

@article{iyengar2005robust,
	title={Robust dynamic programming},
	author={Iyengar, Garud N},
	journal={Mathematics of Operations Research},
	volume={30},
	number={2},
	pages={257--280},
	year={2005},
	publisher={INFORMS}
}

@inproceedings{spibb,
	title={Safe policy improvement with baseline bootstrapping},
	author={Laroche, Romain and Trichelair, Paul and Des Combes, Remi Tachet},
	booktitle={International Conference on Machine Learning},
	pages={3652--3661},
	year={2019}
}

@inproceedings{baird95residual,
	title={Residual algorithms: Reinforcement learning with function approximation},
	author={Baird, L.},
	booktitle={Proceedings of the twelfth international conference on machine learning (ICML 1995)}, 
	pages={30--37},
	year={1995},
}

@inproceedings{teh17distral,
	title={Distral: Robust multitask reinforcement learning},
	author={Teh, Yee and Bapst, Victor and Czarnecki, Wojciech M and Quan, John and Kirkpatrick, James and Hadsell, Raia and Heess, Nicolas and Pascanu, Razvan},
	booktitle={Advances in Neural Information Processing Systems},
	year={2017}
}

@inproceedings{thomas2015high,
	title={High-confidence off-policy evaluation},
	author={Thomas, Philip S and Theocharous, Georgios and Ghavamzadeh, Mohammad},
	booktitle={Twenty-Ninth AAAI Conference on Artificial Intelligence},
	year={2015}
}

@inproceedings{thomas2015high2,
	title={High confidence policy improvement},
	author={Thomas, Philip and Theocharous, Georgios and Ghavamzadeh, Mohammad},
	booktitle={International Conference on Machine Learning},
	pages={2380--2388},
	year={2015}
}


@inproceedings{nadjahi2019safe,
	title={Safe policy improvement with soft baseline bootstrapping},
	author={Nadjahi, Kimia and Laroche, Romain and des Combes, R{\'e}mi Tachet},
	booktitle={Joint European Conference on Machine Learning and Knowledge Discovery in Databases},
	pages={53--68},
	year={2019},
	organization={Springer}
}

@article{simao2019safe,
	title={Safe Policy Improvement with an Estimated Baseline Policy},
	author={Sim{\~a}o, Thiago D and Laroche, Romain and Combes, R{\'e}mi Tachet des},
	journal={arXiv preprint arXiv:1909.05236},
	year={2019}
}

@article{strehl2008analysis,
	title={An analysis of model-based interval estimation for Markov decision processes},
	author={Strehl, Alexander L and Littman, Michael L},
	journal={Journal of Computer and System Sciences},
	volume={74},
	number={8},
	pages={1309--1331},
	year={2008},
	publisher={Academic Press}
}

@article{goyal2018robust,
	title={Robust Markov decision process: Beyond rectangularity},
	author={Goyal, Vineet and Grand-Clement, Julien},
	journal={arXiv preprint arXiv:1811.00215},
	year={2018}
}
@article{hu2020provable,
	title={Provable benefit of orthogonal initialization in optimizing deep linear networks},
	author={Hu, Wei and Xiao, Lechao and Pennington, Jeffrey},
	journal={arXiv preprint arXiv:2001.05992},
	year={2020}
}

@article{imitate,
	title={Imitation learning: A survey of learning methods},
	author={Hussein, Ahmed and Gaber, Mohamed Medhat and Elyan, Eyad and Jayne, Chrisina},
	journal={ACM Computing Surveys (CSUR)},
	volume={50},
	number={2},
	pages={1--35},
	year={2017},
	publisher={ACM New York, NY, USA}
}


@article{smith2006optimizer,
	title={The optimizer’s curse: Skepticism and postdecision surprise in decision analysis},
	author={Smith, James E and Winkler, Robert L},
	journal={Management Science},
	volume={52},
	number={3},
	pages={311--322},
	year={2006},
	publisher={INFORMS}
}

@article{munos2007per,
	title={Performance bounds in l\_p-norm for approximate value iteration},
	author={Munos, R{\'e}mi},
	journal={SIAM journal on control and optimization},
	volume={46},
	number={2},
	pages={541--561},
	year={2007},
	publisher={SIAM}
}


@article{thomas2019preventing,
	title={Preventing undesirable behavior of intelligent machines},
	author={Thomas, Philip S and da Silva, Bruno Castro and Barto, Andrew G and Giguere, Stephen and Brun, Yuriy and Brunskill, Emma},
	journal={Science},
	volume={366},
	number={6468},
	pages={999--1004},
	year={2019},
	publisher={American Association for the Advancement of Science}
}

% Related work in Imaitation learning 

@inproceedings{kim2013learning,
	title={Learning from limited demonstrations},
	author={Kim, Beomjoon and Farahmand, Amir-massoud and Pineau, Joelle and Precup, Doina},
	booktitle={Advances in Neural Information Processing Systems},
	pages={2859--2867},
	year={2013}
}

@inproceedings{chemali2015direct,
	title={Direct policy iteration with demonstrations},
	author={Chemali, Jessica and Lazaric, Alessandro},
	booktitle={IJCAI-24th International Joint Conference on Artificial Intelligence},
	year={2015}
}

@inproceedings{piot2014boosted,
	title={Boosted bellman residual minimization handling expert demonstrations},
	author={Piot, Bilal and Geist, Matthieu and Pietquin, Olivier},
	booktitle={Joint European Conference on Machine Learning and Knowledge Discovery in Databases},
	pages={549--564},
	year={2014},
	organization={Springer}
}

@article{hester2017deep,
	title={Deep q-learning from demonstrations},
	author={Hester, Todd and Vecerik, Matej and Pietquin, Olivier and Lanctot, Marc and Schaul, Tom and Piot, Bilal and Horgan, Dan and Quan, John and Sendonaris, Andrew and Dulac-Arnold, Gabriel and others},
	journal={arXiv preprint arXiv:1704.03732},
	year={2017}
}

@article{vecerik2017leveraging,
	title={Leveraging demonstrations for deep reinforcement learning on robotics problems with sparse rewards},
	author={Vecerik, Mel and Hester, Todd and Scholz, Jonathan and Wang, Fumin and Pietquin, Olivier and Piot, Bilal and Heess, Nicolas and Roth{\"o}rl, Thomas and Lampe, Thomas and Riedmiller, Martin},
	journal={arXiv preprint arXiv:1707.08817},
	year={2017}
}

@inproceedings{ho2016model,
	title={Model-free imitation learning with policy optimization},
	author={Ho, Jonathan and Gupta, Jayesh and Ermon, Stefano},
	booktitle={International Conference on Machine Learning},
	pages={2760--2769},
	year={2016}
}

@article{sun2018truncated,
	title={Truncated horizon policy search: Combining reinforcement learning \& imitation learning},
	author={Sun, Wen and Bagnell, J Andrew and Boots, Byron},
	journal={arXiv preprint arXiv:1805.11240},
	year={2018}
}

@article{evans2015learning,
	title={Learning the preferences of ignorant, inconsistent agents},
	author={Evans, Owain and Stuhlm{\"u}ller, Andreas and Goodman, Noah D},
	journal={arXiv preprint arXiv:1512.05832},
	year={2015}
}

@inproceedings{nair2018overcoming,
	title={Overcoming exploration in reinforcement learning with demonstrations},
	author={Nair, Ashvin and McGrew, Bob and Andrychowicz, Marcin and Zaremba, Wojciech and Abbeel, Pieter},
	booktitle={2018 IEEE International Conference on Robotics and Automation (ICRA)},
	pages={6292--6299},
	year={2018},
	organization={IEEE}
}

% Offline RL
@article{algaedice,
	title={Algaedice: Policy gradient from arbitrary experience},
	author={Nachum, Ofir and Dai, Bo and Kostrikov, Ilya and Chow, Yinlam and Li, Lihong and Schuurmans, Dale},
	journal={arXiv preprint arXiv:1912.02074},
	year={2019}
}

@incollection{gordon1995stable,
	title={Stable function approximation in dynamic programming},
	author={Gordon, Geoffrey J},
	booktitle={Machine Learning Proceedings 1995},
	pages={261--268},
	year={1995},
	publisher={Elsevier}
}
@article{ormoneit2002kernel,
	title={Kernel-based reinforcement learning},
	author={Ormoneit, Dirk and Sen, {\'S}aunak},
	journal={Machine learning},
	volume={49},
	number={2-3},
	pages={161--178},
	year={2002},
	publisher={Springer}
}

@article{ernst2005tree,
	title={Tree-based batch mode reinforcement learning},
	author={Ernst, Damien and Geurts, Pierre and Wehenkel, Louis},
	journal={Journal of Machine Learning Research},
	volume={6},
	number={Apr},
	pages={503--556},
	year={2005}
}

@inproceedings{riedmiller2005neural,
	title={Neural fitted Q iteration--first experiences with a data efficient neural reinforcement learning method},
	author={Riedmiller, Martin},
	booktitle={European Conference on Machine Learning},
	pages={317--328},
	year={2005},
	organization={Springer}
}

@article{rem,
	title={Striving for simplicity in off-policy deep reinforcement learning},
	author={Agarwal, Rishabh and Schuurmans, Dale and Norouzi, Mohammad},
	journal={arXiv preprint arXiv:1907.04543},
	year={2019}
}

@article{vae,
	title={Tutorial on variational autoencoders},
	author={Doersch, Carl},
	journal={arXiv preprint arXiv:1606.05908},
	year={2016}
}

@article{mmd,
	title={A kernel two-sample test},
	author={Gretton, Arthur and Borgwardt, Karsten M and Rasch, Malte J and Sch{\"o}lkopf, Bernhard and Smola, Alexander},
	journal={The Journal of Machine Learning Research},
	volume={13},
	number={1},
	pages={723--773},
	year={2012},
	publisher={JMLR. org}
}

@inproceedings{dualdice,
	title={Dualdice: Behavior-agnostic estimation of discounted stationary distribution corrections},
	author={Nachum, Ofir and Chow, Yinlam and Dai, Bo and Li, Lihong},
	booktitle={Advances in Neural Information Processing Systems},
	pages={2318--2328},
	year={2019}
}


@article{replay_buffer,
	title={Self-improving reactive agents based on reinforcement learning, planning and teaching},
	author={Lin, Long-Ji},
	journal={Machine learning},
	volume={8},
	number={3-4},
	pages={293--321},
	year={1992},
	publisher={Springer}
}

@article{nature_dqn,
	title={Human-level control through deep reinforcement learning},
	author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and others},
	journal={nature},
	volume={518},
	number={7540},
	pages={529--533},
	year={2015},
	publisher={Nature Publishing Group}
}

@inproceedings{policy_gradient,
	title={Policy gradient methods for reinforcement learning with function approximation},
	author={Sutton, Richard S and McAllester, David A and Singh, Satinder P and Mansour, Yishay},
	booktitle={Advances in neural information processing systems},
	pages={1057--1063},
	year={2000}
}

@inproceedings{dpg,
	title={Deterministic policy gradient algorithms},
	author={Silver, David and Lever, Guy and Heess, Nicolas and Degris, Thomas and Wierstra, Daan and Riedmiller, Martin},
	year={2014}
}

@article{ddpg,
	title={Continuous control with deep reinforcement learning},
	author={Lillicrap, Timothy P and Hunt, Jonathan J and Pritzel, Alexander and others},
	journal={arXiv preprint arXiv:1509.02971},
	year={2015}
}

@inproceedings{wang2018exponentially,
	title={Exponentially weighted imitation learning for batched historical data},
	author={Wang, Qing and Xiong, Jiechao and Han, Lei and Liu, Han and Zhang, Tong and others},
	booktitle={Advances in Neural Information Processing Systems},
	pages={6288--6297},
	year={2018}
}

@article{tsne,
	title={Visualizing data using t-SNE},
	author={Maaten, Laurens van der and Hinton, Geoffrey},
	journal={Journal of machine learning research},
	volume={9},
	number={Nov},
	pages={2579--2605},
	year={2008}
}

@article{iqn,
	title={Implicit quantile networks for distributional reinforcement learning},
	author={Dabney, Will and Ostrovski, Georg and Silver, David and Munos, R{\'e}mi},
	journal={arXiv preprint arXiv:1806.06923},
	year={2018}
}


@article{gym,
	title={Openai gym},
	author={Brockman, Greg and Cheung, Vicki and Pettersson, Ludwig and others},
	journal={arXiv preprint arXiv:1606.01540},
	year={2016}
}

@article{implementation,
	title={Implementation Matters in Deep Policy Gradients: A Case Study on PPO and TRPO},
	author={Engstrom, Logan and Ilyas, Andrew and Santurkar, Shibani and Tsipras, Dimitris and Janoos, Firdaus and Rudolph, Larry and Madry, Aleksander},
	journal={arXiv preprint arXiv:2005.12729},
	year={2020}
}
@article{drl_matters,
	title={Deep reinforcement learning that matters},
	author={Henderson, Peter and Islam, Riashat and Bachman, Philip and Pineau, Joelle and Precup, Doina and Meger, David},
	journal={arXiv preprint arXiv:1709.06560},
	year={2017}
}

@article{d4rl,
	title={D4rl: Datasets for deep data-driven reinforcement learning},
	author={Fu, Justin and Kumar, Aviral and Nachum, Ofir and Tucker, George and Levine, Sergey},
	journal={arXiv preprint arXiv:2004.07219},
	year={2020}
}

@article{bcqbechmarking,
	title={Benchmarking batch deep reinforcement learning algorithms},
	author={Fujimoto, Scott and Conti, Edoardo and Ghavamzadeh, Mohammad and Pineau, Joelle},
	journal={arXiv preprint arXiv:1910.01708},
	year={2019}
}

@article{c51,
	title={A distributional perspective on reinforcement learning},
	author={Bellemare, Marc G and Dabney, Will and Munos, R{\'e}mi},
	journal={arXiv preprint arXiv:1707.06887},
	year={2017}
}

@inproceedings{resnet,
	title={Deep residual learning for image recognition},
	author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
	pages={770--778},
	year={2016}
}

@article{gpt-3,
	title={Language models are few-shot learners},
	author={Brown, Tom B and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
	journal={arXiv preprint arXiv:2005.14165},
	year={2020}
}

@article{alpha,
	title={A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play},
	author={Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and others},
	journal={Science},
	volume={362},
	number={6419},
	pages={1140--1144},
	year={2018},
	publisher={American Association for the Advancement of Science}
}

@article{starcraft,
	title={Grandmaster level in StarCraft II using multi-agent reinforcement learning},
	author={Vinyals, Oriol and Babuschkin, Igor and Czarnecki, Wojciech M and Mathieu, Micha{\"e}l and Dudzik, Andrew and Chung, Junyoung and Choi, David H and Powell, Richard and Ewalds, Timo and Georgiev, Petko and others},
	journal={Nature},
	volume={575},
	number={7782},
	pages={350--354},
	year={2019},
	publisher={Nature Publishing Group}
}

@article{td3,
	title={Addressing function approximation error in actor-critic methods},
	author={Fujimoto, Scott and Van Hoof, Herke and Meger, David},
	journal={arXiv preprint arXiv:1802.09477},
	year={2018}
}
@article{wd3,
	title={Reducing Estimation Bias via Weighted Delayed Deep Deterministic Policy Gradient},
	author={He, Qiang and Hou, Xinwen},
	journal={arXiv preprint arXiv:2006.12622},
	year={2020}
}

@article{gao2018reinforcement,
	title={Reinforcement learning from imperfect demonstrations},
	author={Gao, Yang and Xu, Huazhe and Lin, Ji and Yu, Fisher and Levine, Sergey and Darrell, Trevor},
	journal={arXiv preprint arXiv:1802.05313},
	year={2018}
}

@article{wang2000class,
	title={A class of distortion operators for pricing financial and insurance risks},
	author={Wang, Shaun S},
	journal={Journal of risk and insurance},
	pages={15--36},
	year={2000},
	publisher={JSTOR}
}

@article{OUprocess,
	title={On the theory of the Brownian motion},
	author={Uhlenbeck, George E and Ornstein, Leonard S},
	journal={Physical review},
	volume={36},
	number={5},
	pages={823},
	year={1930},
	publisher={APS}
}

@article{sac,
	title={Soft actor-critic algorithms and applications},
	author={Haarnoja, Tuomas and Zhou, Aurick and Hartikainen, Kristian and others},
	journal={arXiv preprint arXiv:1812.05905},
	year={2018}
}

@inproceedings{wu2019hierarchical,
  title={Hierarchical macro strategy model for moba game ai},
  author={Wu, Bin},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={33},
  pages={1206--1213},
  year={2019}
}

@article{fortunato2017noisy,
  title={Noisy networks for exploration},
  author={Fortunato, Meire and Azar, Mohammad Gheshlaghi and Piot, Bilal and others},
  journal={arXiv preprint arXiv:1706.10295},
  year={2017}
}

@article{he2020popo,
  title={POPO: Pessimistic Offline Policy Optimization},
  author={He, Qiang and Hou, Xinwen},
  journal={arXiv preprint arXiv:2012.13682},
  year={2020}
}

@inproceedings{iqn ,
  title={Implicit quantile networks for distributional reinforcement learning},
  author={Dabney, Will and Ostrovski, Georg and Silver, David and Munos, R{\'e}mi},
  booktitle={International conference on machine learning},
  pages={1096--1105},
  year={2018},
  organization={PMLR}
}

@inproceedings{adt,
  title={Adversarial Policy Training against Deep Reinforcement Learning},
  author={Wu, Xian and Guo, Wenbo and Wei, Hua and Xing, Xinyu},
  booktitle={30th $\{$USENIX$\}$ Security Symposium ($\{$USENIX$\}$ Security 21)},
  year={2021}
}

@article{gleave2019adversarial,
  title={Adversarial policies: Attacking deep reinforcement learning},
  author={Gleave, Adam and Dennis, Michael and Wild, Cody and Kant, Neel and Levine, Sergey and Russell, Stuart},
  journal={arXiv preprint arXiv:1905.10615},
  year={2019}
}

@article{xiao2019characterizing,
  title={Characterizing attacks on deep reinforcement learning},
  author={Xiao, Chaowei and Pan, Xinlei and He, Warren and Peng, Jian and Sun, Mingjie and Yi, Jinfeng and Liu, Mingyan and Li, Bo and Song, Dawn},
  journal={arXiv preprint arXiv:1907.09470},
  year={2019}
}

@article{wang2016learning,
  title={Learning to reinforcement learn},
  author={Wang, Jane X and Kurth-Nelson, Zeb and Tirumala, Dhruva and Soyer, Hubert and Leibo, Joel Z and Munos, Remi and Blundell, Charles and Kumaran, Dharshan and Botvinick, Matt},
  journal={arXiv preprint arXiv:1611.05763},
  year={2016}
}