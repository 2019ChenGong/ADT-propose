\documentclass[a4paper]{article}
\usepackage[UTF8]{ctex}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{url}
\usepackage{multirow}
\usepackage{array}
\usepackage{booktabs}
\usepackage{url}
\usepackage{enumitem}
\usepackage{graphicx}
\usepackage{float}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{subfig}
\usepackage{longtable}
\usepackage{pifont}
\usepackage{color}
\usepackage{cite}

\allowdisplaybreaks

\geometry{a4paper, scale=0.78}

\usepackage{tikz}
\newcommand*{\circled}[1]{\lower.7ex\hbox{\tikz\draw (0pt, 0pt)%
    circle (.5em) node {\makebox[1em][c]{\small #1}};}}
\newcommand{\RNum}[1]{\uppercase\expandafter{\romannumeral #1\relax}}

% \begin{figure}[H]
%     \centering
%     \includegraphics[width=.55\textwidth]{E.png}
%     \caption{矩阵与列向量的乘法}
%     \label{fig:my_label_1}
% \end{figure}

% \left\{
% \begin{array}{ll}
%       x+2x+z=2 & \\
%       3x+8y+z=12 & \\
%       4y+z=2
% \end{array}
% \right.

% \begin{enumerate}[itemindent = 1em, itemsep = 0.4pt, parsep=0.5pt, topsep = 0.5pt]

% \end{enumerate}

%\stackrel{a}{\longrightarrow}

%\underbrace{}_{} %下括号

% \tableofcontents %目录，并且目录页不记录页码
% \tableofcontents
% \newpage
% \setcounter{page}{1} %new page
% \clearpage

\title{Adversarial Policy Training against Deep Reinforcement Learning}
\author{Chen Gong}
\date{12 May 2021}

\begin{document}
\maketitle

\section{Summary}
Currently, Deep Reinforcement Learning (DRL) has been widely applied in various fields, 
and shows superior performance on video games, such as Atari 2600 games \cite{silver_mastering_2016}, Go \cite{mnih_human-level_2015}, StarCraft \RNum{2} \cite{vinyals2019grandmaster} and so on.
Besides, combining DRL with adversarial attacks, 
an adversary could force a well-trained agent to behave abnormally by perturbing the input of agent’s
policy network or training an adversarial agent to exploit the weakness of the victim \cite{adt}.

In this paper, authors extends a reinforcement learning algorithm (Proximal Policy Optimization, PPO) to guide the training of the adversarial agent in the two-agent competitive game setting.
The main contributions are concluded as follows,
\begin{enumerate}
    \item \textbf{Strong Practicality:} In many recent research works, they assume an attack has the privilege to manipulate the environment freely, and explicit knowledge to the opponent agent policy \cite{xiao2019characterizing}. 
    However, these attacks are not practical in the real world. Although the new attack against RL is designed without manipulation of environment\cite{vinyals2019grandmaster}, 
    this newly method usually shows a relatively low success rate of failing the opponent agent. This is because the attack is a simple application of the PPO, and PPO does not train an agent for exploiting the weakness of the opponent agent.
    
    The attack mechanism is the first work that can effectively exploit the weakness of victim agents without manipulation of the environment, explicit knowledge of the opponent policy network and state-transition model.
    So this attack mechanism is practical, and trains an adversarial agent in an effective and efficient fashion. 
    \item \textbf{Explainable AI Technique:} To facilitate the search of the adversarial policy network, 
    authors adjust the weight of the action deviation based on by how much the victim agent pays attention to the action of the action of the adversarial.
    The weight is estimated by an explainable AI technique.
    \item \textbf{SOTA Performance:} Authors conduct experiments using MuJoCo and roboschool Pong to evaluation their attack. Compared with the SOTA technique \cite{gleave2019adversarial},
    the adversarial agent exhibits a much stronger capability in exploiting the weakness of victim agents (an average of 60\% vs. 50\% winning rate for
    MuJoCo game and 100\% vs. 90\% for the Pong game). The attack method in this paper can conduct an adversarial agent with a 50\% winning rate in fewer sampling (11 million vs. 20
    million iterations for MuJoCo game, and 1.0 million vs. 1.3 million iterations for Pong game).
    Besides, adversarial attack shows less variation in the training process.
\end{enumerate}

This paper extends a SOTA reinforcement learning algorithm to guide the training of the adversarial agent in the two-agent competitive game setting. However, there are also some disadvantages: 
The attack method can only be applied to the two-agent competitive game setting.




\section{My Reflections}
\begin{enumerate}
    \item \textbf{Distributional Reinforcement Learning:} Value distribution means the distribution of the random return received by the distribution of the random return received by a reinforcement learning agent. 
    This is in contrast to the common approach to reinforcement learning which models the expectation of this return, or value \cite{bellemare_distributional_2017}. 
    Distributional reinforcement learning focuses on the learning of value distribution, which has many advantages: 1. it applies more decision information (i.e., for some risk-sensitive tasks, we may choose actions that have a lower variance or a better worst-case scenario than actions that have a higher mean); 2. it alleviates the problem of reward sparsity, and sparse rewards are more likely to be retained during propagation;
    3. for some states that appear to be the same in representation, there may be two completely different value functions, and if only the mean value is considered, the information will be completely confused (i.e., $\mathcal{N}(0,0.1)$ and $\mathcal{N}(0,10)$).

    Fortunately, the advantages of distributional reinforcement learning can solve many problems in adversarial attack, so maybe distributional reinforcement learning can achieve better performance in adversarial attack.
    \item \textbf{Meta Reinforcement Learning: }there is a problem that whether an adversarial policy network trained against one practical opponent agent could also be used to defeat the other agents trained differently after simple training, but serving for the same reinforcement learning task. 
    Meta reinforcement learning (Meta RL) may be an ideal solution. Meta-RL is meta-learning on reinforcement learning tasks. After trained over a distribution of tasks, the agent is able to solve a new task by developing a new RL algorithm.
    
    A good meta-learning model is expected to generalize to new tasks or new environments that have never been encountered during training. 
    The meta RL includes two sections, meta-training and meta-adaptation. Meta learning process learns knowledge from the past Markov decision process.
    Meta adaptation is that how to quickly change the network to adapt a new task \cite{wang2016learning}. 
    So in our problem, meta RL algorithms can make an adversarial agent attack different opponent agents in the same environment to learning the meta knowledge,
    and the adversarial agent defeats an opponent agent that it never encounters after simple training.
    
    \item \textbf{Multi-Agent Reinforcement Learning: }we can extend the attack mechanism to multi-agent environments, where multiple participants collaborate or compete with each other.
    When multiple agents interact with the environment at the same time, the system becomes a multi-agent system. Each agent still follows the goal of reinforcement learning, that is, to maximize the cumulative return. 
    The transition of the state of the environment is related to the joint actions of all agents so that the influence of the joint actions should be considered.
    Can we extend the multi-agent RL algorithms to study the attack methods in multi-agent environments?
\end{enumerate}

\clearpage
\bibliographystyle{ieeetran}
\bibliography{mybib}

\section*{Appendix}
In this section, I introduce the details in algorithm. This method extend the PPO loss function $L_\text{PPO}$ a new loss term
\begin{equation}
    L_\text{ad}=\operatorname{maximize}_{\theta}\left(\left\|\hat{a}_{v}^{(t+1)}-a_{v}^{(t+1)}\right\|_{1}-\left\|\hat{o}_{v}^{(t+1)}-o_{v}^{(t+1)}\right\|_{1}\right),
\end{equation}
where $\theta$ is parameters in adversarial agent $\pi_\alpha$.
$\hat{o}_v^{(t+1)}$ and $\hat{a}_v^{(t+1)}$ are the different observation and action taken by the opponent agent if, 
at the time step $t$, the adversarial agent takes an action different from the ones indicated by the trajectory rollouts.
The key idea of this term can be concluded as, 
when maximizing the deviation of an opponent action,
we need to ensure the minimal change of the environment observation. 

Neither the opponent policy network $\pi_v$ nor state-transition model $\mathcal{P}_v^{ss'}$,
we cannot get the observation of the opponent agent $\hat{o}_v^{(t+1)}$ and the action of the opponent agent $\hat{a}_v^{(t+1)} = \pi_v(\hat{o}_v^{(t+1)})$.
Therefore, author build two individual deep nature networks to approximate the opponent policy network and its state-transition model.

The state-transition model is defined as $H(o_v^{(t)},a_v^{(t)},a_\alpha^{(t)}; \theta_h)$, where $(o_v^{(t)},a_v^{(t)},a_\alpha^{(t)})$ is input and $o_v^{(t+1)}$ is output.
So the $\hat{a}_{v}^{(t+1)}$ is the output of $H(o_v^{(t)},a_v^{(t)},\hat{a}_\alpha^{(t)}; \theta_h)$.

The policy opponent policy model is defined as $F(o_v^{(t)}; \theta_f)$, where $(o_v^{(t)})$ is input and $a_v^{(t+1)}$ is output.
So the $\hat{a}_{v}^{(t+1)}$ is the output of $F(H(o_v^{(t)},a_v^{(t)},\hat{a}_\alpha^{(t)}; \theta_h);\theta_f)$. 

The $L_\text{ad}$ can be rewritten as
\begin{equation}
    \begin{aligned}
        L_{a d}= \operatorname{maximize}_{\theta}\left(\left\|F\left(H\left(o_{v}^{(t)}, a_{v}^{(t)}, \hat{a}_{\alpha}^{(t)}\right)\right)-a_{v}^{(t+1)}\right\|_{1}-\left\|H\left(o_{v}^{(t)}, a_{v}^{(t)}, \hat{a}_{\alpha}^{(t)}\right)-o_{v}^{(t+1)}\right\|_{1}\right)
    \end{aligned}    
\end{equation}

Obviously, the total loss function is defined as $L_\text{PPO} + \lambda L_\text{ad}$. The hyperparameter $\lambda$ indicate the importance of newly added term $L_\text{ad}$, 
which is calculated based on the weight that the opponent agent pays attention to the adversarial. 
The $\lambda$ in step time $t$ is defined as
\begin{equation}
    I^{(t)}=\left\|F\left(o_{v}^{(t)}\right)-F\left(o_{v}^{(t)} \odot\left(\tilde{g}^{(t)} \odot M\right)\right)\right\|_{\infty},\quad \lambda^{(t)}=\frac{1}{1+I^{(t)}},
\end{equation}
where $\tilde{g}^{(t)} = \sum_{j=1:q} g_{ij}^{(t)}$, and $g^{(t)_{ij}} = \nabla_{(o^{(t)}_i} F(o^{(t)}_v)_j$. 
In $o^{(t)}_v$, if the corresponding observation dimensions indicate the actions of adversarial agent, we assign 1 to the
corresponding element in $M$, and the rest is assign to 0.

\end{document}
